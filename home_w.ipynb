{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forced-danger",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef33c7-3a0f-48e2-b2ed-8b92170d46d2",
   "metadata": {},
   "source": [
    "## Пункт 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0ce07c-1071-4b14-99f0-54a2a3b31990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contrary-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение книги из файла и сохранение ее по параграфам\n",
    "with open('book.txt', encoding='utf-8') as book:\n",
    "    book = book.read()\n",
    "    \n",
    "    paragraphs = [paragraph for paragraph in book.split('\\n') if len(paragraph)>0] \n",
    "    text = ''\n",
    "    \n",
    "    \n",
    "# обработка параграфов\n",
    "for paragraph in paragraphs:\n",
    "#     оставляем только слова\n",
    "    lemmas = [word for word in m.lemmatize(paragraph) if (len(re.findall(r'[а-яё]', word)) > 0)]\n",
    "    \n",
    "    for lemma in lemmas:\n",
    "#         формируем текст с пробелами после слов и переносом строки после абзацев\n",
    "        text += lemma + ' '\n",
    "    \n",
    "    text = text[:len(text) - 1] + '\\n'\n",
    "\n",
    "    \n",
    "# записываем обработанные данные\n",
    "with open('new_book.txt', 'w', encoding='utf-8') as new_file:\n",
    "    new_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a100b-53ec-48b9-b784-846546a6b4ef",
   "metadata": {},
   "source": [
    "#### результат сохранен в файле new_book.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-landscape",
   "metadata": {},
   "source": [
    "## Пункт 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8967d814-8d8f-4716-8a55-f9337577c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import jsonlines\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd25511-ef5b-47e6-90bd-f4ed92e517bc",
   "metadata": {},
   "source": [
    "### Если при запуске следующего блока кода появляется ошибка с nltk word_tokenize, необходимо раскомментировать и запустить последующий блок кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separated-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# из токенов оставляем только слова\n",
    "# (словом считается та строка, в которой содержится хотя бы одна русская буква)\n",
    "tokens = word_tokenize(book)\n",
    "words = [word for word in tokens if (len(re.findall(r'[А-ЯЁа-яё]', word))>0)]\n",
    "\n",
    "# создаем список, в котором буду лежать словари для каждого слова\n",
    "dict_list = []\n",
    "for word in words:\n",
    "    dct = {}\n",
    "    data = morph.parse(word)[0]\n",
    "    dct['lemma'] = data.normal_form\n",
    "    dct['word'] = word\n",
    "    dct['pos'] = data.tag.POS\n",
    "    dict_list.append(dct)\n",
    "\n",
    "# записываем в файл с помощью  jsonlines\n",
    "with open('tokens_dict_book.jsonl', 'w',encoding = 'utf-8') as output:\n",
    "    with jsonlines.Writer(output) as writer:\n",
    "        writer.write_all(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d39f9d-b146-4b91-9ce7-77e71fc551b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771ba4e-09b7-47b5-8742-05075c8e2dc2",
   "metadata": {},
   "source": [
    "#### результат сохранен в файле tokens_dict_book.jsonl¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-barrier",
   "metadata": {},
   "source": [
    "## Пункт 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f0a60-d812-48b9-873d-16a8efc177ed",
   "metadata": {},
   "source": [
    "### Часть 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38699b18-c033-4b12-9cba-cf63dbebb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conceptual-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ:\n",
      "\n",
      "NOUN: 30.74 %\n",
      "ADJF: 11.8 %\n",
      "VERB: 12.7 %\n",
      "PREP: 10.78 %\n",
      "CONJ: 9.8 %\n",
      "ADVB: 6.32 %\n",
      "GRND: 0.85 %\n",
      "NPRO: 5.73 %\n",
      "PRCL: 4.88 %\n",
      "PRTF: 0.95 %\n",
      "INFN: 2.54 %\n",
      "ADJS: 0.63 %\n",
      "NUMR: 0.78 %\n",
      "COMP: 0.56 %\n",
      "PRED: 0.28 %\n",
      "PRTS: 0.58 %\n",
      "INTJ: 0.02 %\n",
      "None: 0.06 %\n"
     ]
    }
   ],
   "source": [
    "pos = [word['pos'] for word in dict_list]\n",
    "\n",
    "# создаем списки для каждой части речи\n",
    "print('Ответ:\\n')\n",
    "\n",
    "# рассчитываем и выводим долю слов каждой части речи в процентном соотношении\n",
    "for key, val in Counter(pos).items():\n",
    "    #обсчитываю с помощью counter\n",
    "    print(key, ': ', round((val / len(pos)) * 100, 2), ' %', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-bunch",
   "metadata": {},
   "source": [
    "### Подпункт 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b276a-e7a2-4e3a-93d5-3f3ef3ea26bb",
   "metadata": {},
   "source": [
    "#### глаголы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "independent-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "быть\n",
      "стать\n",
      "мочь\n",
      "сказать\n",
      "говорить\n",
      "казаться\n",
      "видеть\n",
      "объявить\n",
      "знать\n",
      "оказаться\n",
      "помнить\n",
      "выйти\n",
      "верить\n",
      "хотеть\n",
      "работать\n",
      "пить\n",
      "думать\n",
      "следовать\n",
      "сделать\n",
      "стоять\n"
     ]
    }
   ],
   "source": [
    "# составляем список глаголов\n",
    "verbs = [word['lemma'] for word in dict_list if word['pos'] == 'VERB']\n",
    "\n",
    "# сортируем список от большего к меньшему (порядок обозначетсязначениемв Counter, то есть кол-вом употреблений)\n",
    "vlist = sorted(Counter(verbs).keys(), key=Counter(verbs).get, reverse=True)\n",
    "\n",
    "# выводим первые 20\n",
    "for i in range(20):\n",
    "    print(vlist[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-salon",
   "metadata": {},
   "source": [
    "#### наречия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "processed-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ещё\n",
      "только\n",
      "уже\n",
      "теперь\n",
      "несколько\n",
      "очень\n",
      "где\n",
      "никогда\n",
      "всегда\n",
      "там\n",
      "почти\n",
      "тут\n",
      "потом\n",
      "хорошо\n",
      "вдруг\n",
      "сразу\n",
      "наконец\n",
      "иногда\n",
      "вместе\n",
      "ничего\n"
     ]
    }
   ],
   "source": [
    "# аналогично глаголам\n",
    "adv = [word['lemma'] for word in dict_list if word['pos'] == 'ADVB']\n",
    "advlist = sorted(Counter(adv).keys(), key=Counter(adv).get, reverse=True)\n",
    "\n",
    "for i in range(20):\n",
    "    print(advlist[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-usage",
   "metadata": {},
   "source": [
    "## Пункт 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727485d3-60ea-46f7-ae24-9797ee90aab6",
   "metadata": {},
   "source": [
    "### Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420b608e-95b0-4883-b5b5-f739b90a0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advance-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('на', 'ферма')\n",
      "('то', 'что')\n",
      "('не', 'быть')\n",
      "('зверский', 'ферма')\n",
      "('что', 'они')\n",
      "('они', 'не')\n",
      "('и', 'не')\n",
      "('не', 'мочь')\n",
      "('что', 'он')\n",
      "('мистер', 'джонс')\n",
      "('никогда', 'не')\n",
      "('весь', 'животное')\n",
      "('в', 'этот')\n",
      "('в', 'свой')\n",
      "('это', 'быть')\n",
      "('бывший', 'кабан')\n",
      "('он', 'быть')\n",
      "('свинья', 'и')\n",
      "('и', 'даже')\n",
      "('ферма', 'и')\n",
      "('и', 'в')\n",
      "('в', 'дом')\n",
      "('они', 'быть')\n",
      "('что', 'в')\n",
      "('говорить', 'что')\n"
     ]
    }
   ],
   "source": [
    "# считываем созданный ранее файл с леммами без пунктуации\n",
    "with open('new_book.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.split()\n",
    "# создем отсортированный от большего к меньшему список биграмм (ключ - значение в Counter, то есть количество употреблений)\n",
    "text_bigrams = sorted(Counter(bigrams(text)).keys(), key = Counter(bigrams(text)).get, reverse = True)\n",
    "\n",
    "# выводим первые 25\n",
    "for i in range(25):\n",
    "    print(text_bigrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-insider",
   "metadata": {},
   "source": [
    "### Триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bf8ba9-0de3-4bb3-b615-a1f5b61b52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('весь', 'животное', 'британия')\n",
      "('битва', 'при', 'коровник')\n",
      "('на', 'задний', 'ножка')\n",
      "('в', 'то', 'что')\n",
      "('о', 'то', 'что')\n",
      "('четыре', 'хорошо', 'два')\n",
      "('бывший', 'кабан', 'борька')\n",
      "('снежок', 'и', 'наполеон')\n",
      "('зверь', 'да', 'не')\n",
      "('у', 'он', 'быть')\n",
      "('боксер', 'и', 'кашка')\n",
      "('хорошо', 'два', 'плохо')\n",
      "('в', 'конец', 'конец')\n",
      "('в', 'битва', 'при')\n",
      "('свинья', 'и', 'пес')\n",
      "('на', 'большой', 'гумно')\n",
      "('один', 'из', 'они')\n",
      "('с', 'тот', 'пора')\n",
      "('как', 'раз', 'в')\n",
      "('весь', 'зверь', 'равный')\n",
      "('в', 'этот', 'минута')\n",
      "('я', 'быть', 'работать')\n",
      "('быть', 'работать', 'еще')\n",
      "('работать', 'еще', 'упорно')\n",
      "('зверский', 'ферма', 'и')\n"
     ]
    }
   ],
   "source": [
    "# аналогично биграммам\n",
    "text_trigrams = sorted(Counter(trigrams(text)).keys(), key = Counter(trigrams(text)).get, reverse = True)\n",
    "\n",
    "for i in range(25):\n",
    "    print(text_trigrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a423173-0f07-4498-a68f-8cc96bfa3f64",
   "metadata": {},
   "source": [
    "#### тут будет ответ про би и три граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-median",
   "metadata": {},
   "source": [
    "## Пункт 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c554a-64ed-49e1-8bbe-26b54d3f842f",
   "metadata": {},
   "source": [
    "В данной задаче важно понимать, что не все глаголы имеют все 3 формы и не все существительные могут употребляться во множественном и единственном числе. Так что программа подобные случаи типа pluralia / singularia tantum не обрабатывает.\n",
    "(Подобные ошибки особо заметны на примере \"миссис Джонс\" -> \"миссис Джонсы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collect-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "владельцы Барской ферм мистеры джонсы позапираю  на ночи курятник, но  о цыплячьих лазе спьяну забуду. Пошатываясь и рисуя на землях петель лучами света от фонариков, он пересеку дворы, скину сапог у  заднего крылец, нацежу себе еще одну кружки пива из бочонков в буфетной при кухнях и завалюсь на кровати, в которой уже похрапываю миссис джонсы.\n",
      "Лишь только  свет  в спальнях погасну, вся усадьбы приду в  движения. \n",
      "Еще днями по фермах пронесусь слухи, будто прошлой ночию старый майоры, премированный хряки  средней белой  порода, вижу поразительный сны и желал поведать о нем другим животному.\n"
     ]
    }
   ],
   "source": [
    "# указываем текст для обработки\n",
    "paragraph = '''Владелец Барской Фермы мистер Джонс позапирал  на ночь курятники, но  о цыплячьих лазах спьяну забыл. Пошатываясь и рисуя на земле петли лучом света от фонарика, он пересек двор, скинул сапоги у  заднего крыльца, нацедил себе еще одну кружку пива из бочонка в буфетной при кухне и завалился на кровать, в которой уже похрапывала миссис Джонс.\n",
    "Лишь только  свет  в спальне погас, вся усадьба пришла в  движение. \n",
    "Еще днем по ферме пронесся слух, будто прошлой ночью старый Майор, премированный хряк  средней белой  породы, видел поразительный сон и желает поведать о нем другим животным.'''\n",
    "\n",
    "# разбиваем текст на токены для анализа\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "# обрабатывает токены-слова\n",
    "for word in words:\n",
    "    new_word = word\n",
    "    data = morph.parse(word)[0]\n",
    "\n",
    "#     для глаголов прошедшее меняется на настоящее или будущее, а непрошедшее меняется на прошедшее\n",
    "    if data.tag.POS == 'VERB':\n",
    "        if data.tag.tense == 'past':\n",
    "            if data.inflect({'pres'}):\n",
    "                new_word = data.inflect({'pres'}).word\n",
    "            else:\n",
    "                new_word = data.inflect({'futr'}).word\n",
    "        else:\n",
    "            new_word = data.inflect({'past'}).word\n",
    "            \n",
    "#     для существительных единственное чсило меняется на множественное и наоборот\n",
    "    if data.tag.POS == 'NOUN':\n",
    "        if data.tag.number == 'sing' and data.inflect({'plur'}):\n",
    "            new_word = data.inflect({'plur'}).word\n",
    "        elif data.inflect({'sing'}):\n",
    "            new_word = data.inflect({'sing'}).word\n",
    "            \n",
    "    #заменяем в тексте старые слова на новые\n",
    "    paragraph = paragraph.replace(word, new_word)\n",
    "\n",
    "# выводим текст\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b46539-8104-4ef9-b7f8-485bdeb3d30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
